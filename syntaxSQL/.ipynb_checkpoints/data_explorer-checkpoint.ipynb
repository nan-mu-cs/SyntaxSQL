{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, count\n",
    "from six import string_types\n",
    "import torch\n",
    "import torchtext.data\n",
    "import torchtext.vocab\n",
    "\n",
    "import table\n",
    "import table.IO\n",
    "import opts\n",
    "from tree import SCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
    "NEW_WHERE_OPS = ('=','>','<','>=','<=','!=','like','not in','in','between','is')\n",
    "NEW_WHERE_DICT = {\n",
    "    '=': 0,\n",
    "    '>': 1,\n",
    "    '<': 2,\n",
    "    '>=': 3,\n",
    "    '<=': 4,\n",
    "    '!=': 5,\n",
    "    'like': 6,\n",
    "    'not in': 7,\n",
    "    'in': 8,\n",
    "    'between': 9,\n",
    "    'is':10\n",
    "}\n",
    "# SQL_OPS = ('none','intersect', 'union', 'except')\n",
    "SQL_OPS = {\n",
    "    'none': 0,\n",
    "    'intersect': 1,\n",
    "    'union': 2,\n",
    "    'except': 3\n",
    "}\n",
    "KW_DICT = {\n",
    "    'where': 0,\n",
    "    'groupBy': 1,\n",
    "    'orderBy': 2\n",
    "}\n",
    "ORDER_OPS = {\n",
    "    'desc': 0,\n",
    "    'asc': 1}\n",
    "AGG_OPS = ('none','max', 'min', 'count', 'sum', 'avg')\n",
    "\n",
    "COND_OPS = {\n",
    "    'and':0,\n",
    "    'or':1\n",
    "}\n",
    "\n",
    "TRAIN_COMPONENTS = ('multi_sql','keyword','col','op','agg','root_tem','des_asc','having','andor','value')\n",
    "COMPONENTS_DICT = {\n",
    "    'multi_sql':0,\n",
    "    'keyword':1,\n",
    "    'col':2,\n",
    "    'op':3,\n",
    "    'agg':4,\n",
    "    'root_tem':5,\n",
    "    'des_asc':6,\n",
    "    'having':7,\n",
    "    'andor':8,\n",
    "    'value':9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_op_index(is_not,op):\n",
    "    op = OLD_WHERE_OPS[op]\n",
    "    if is_not and op == \"in\":\n",
    "        return 7\n",
    "    try:\n",
    "        return NEW_WHERE_DICT[op]\n",
    "    except:\n",
    "        print(\"Unsupport op: {}\".format(op)) # TODO: check ! =\n",
    "        return -1\n",
    "\n",
    "def index_to_column_name(index, table):\n",
    "    column_name = table[\"column_names\"][index][1]\n",
    "    table_index = table[\"column_names\"][index][0]\n",
    "    table_name = table[\"table_names\"][table_index]\n",
    "    return table_name, column_name, index\n",
    "\n",
    "\n",
    "def get_label_cols(with_join,fk_dict,labels):\n",
    "    # list(set([l[1][i][0][2] for i in range(min(len(l[1]), 3))]))\n",
    "    cols = set()\n",
    "    ret = []\n",
    "    for i in range(len(labels)):\n",
    "        cols.add(labels[i][0][2]) # still col index\n",
    "        if len(cols) > 3:\n",
    "            break\n",
    "    for col in cols:\n",
    "        # ret.append([col])\n",
    "        if with_join and len(fk_dict[col]) > 0:\n",
    "            ret.append([col]+fk_dict[col])\n",
    "        else:\n",
    "            ret.append(col)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# history added\n",
    "class MultiSqlPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.keywords = ('intersect', 'except', 'union')\n",
    "\n",
    "    def generate_output(self):\n",
    "        for key in self.sql:\n",
    "            if key in self.keywords and self.sql[key]:\n",
    "                return self.history + ['root'], key, self.sql[key]\n",
    "        return self.history + ['root'], 'none', self.sql\n",
    "\n",
    "\n",
    "class KeyWordPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'limit', 'having')\n",
    "\n",
    "    def generate_output(self):\n",
    "        sql_keywords = []\n",
    "        for key in self.sql:\n",
    "            if key in self.keywords and self.sql[key]: # included other keywords\n",
    "                sql_keywords.append(key)\n",
    "        return self.history, [len(sql_keywords), sql_keywords], self.sql\n",
    "\n",
    "\n",
    "# history added\n",
    "class ColPredictor:\n",
    "    def __init__(self, question, sql, table, history, kw=None):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "        self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'having')\n",
    "        self.kw = kw\n",
    "\n",
    "    def generate_output(self):\n",
    "        ret = []\n",
    "        candidate_keys = self.sql.keys()\n",
    "        if self.kw:\n",
    "            candidate_keys = [self.kw]\n",
    "        for key in candidate_keys:\n",
    "            if key in self.keywords and self.sql[key]:\n",
    "                cols = []\n",
    "                sqls = []\n",
    "                if key == 'groupBy':\n",
    "                    sql_cols = self.sql[key]\n",
    "                    for col in sql_cols:\n",
    "                        cols.append((index_to_column_name(col[1], self.table), col[2]))\n",
    "                        sqls.append(col) # col_unit1\n",
    "                elif key == 'orderBy':\n",
    "                    sql_cols = self.sql[key][1]\n",
    "                    for col in sql_cols: # only contain col_unit1 in val_unit: (unit_op, col_unit1, col_unit2)\n",
    "                        cols.append((index_to_column_name(col[1][1], self.table), col[1][2]))\n",
    "                        sqls.append(col) # val_unit1\n",
    "                elif key == 'select':\n",
    "                    sql_cols = self.sql[key][1]\n",
    "                    for col in sql_cols:  # only contain col_unit1 in val_unit\n",
    "                        cols.append((index_to_column_name(col[1][1][1], self.table), col[1][1][2]))\n",
    "                        sqls.append(col) # (agg_id, val_unit)\n",
    "                elif key == 'where' or key == 'having':\n",
    "                    sql_cols = self.sql[key]\n",
    "                    for col in sql_cols: # TODO: check this one!\n",
    "                        if not isinstance(col, list):\n",
    "                            continue\n",
    "                        try: # col_id of col_unit of val_unit of cond_unit of condition\n",
    "                            cols.append((index_to_column_name(col[2][1][1], self.table), col[2][1][2]))\n",
    "                        except:\n",
    "                            print(\"Key:{} Col:{} Question:{}\".format(key, col, self.question))\n",
    "                        sqls.append(col) # cond_unit\n",
    "                ret.append((\n",
    "                    self.history + [key], (len(cols), cols), sqls\n",
    "                ))\n",
    "        return ret\n",
    "        # ret.append(history+[key],)\n",
    "\n",
    "\n",
    "class OpPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql # check sql is cond_unit\n",
    "        self.question = question\n",
    "        self.history = history # history not change\n",
    "        # self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'having')\n",
    "\n",
    "    def generate_output(self): # sql3: val_unit, sql4: val1\n",
    "        return self.history, convert_to_op_index(self.sql[0],self.sql[1]), (self.sql[3], self.sql[4])\n",
    "\n",
    "\n",
    "class AggPredictor:\n",
    "    def __init__(self, question, sql, history,kw=None):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.kw = kw\n",
    "    def generate_output(self):\n",
    "        label = -1\n",
    "        if self.kw:\n",
    "            key = self.kw\n",
    "        else:\n",
    "            key = self.history[-2]\n",
    "        if key == 'select':\n",
    "            label = self.sql[0] # check sql: (agg_id, val_unit)\n",
    "        elif key == 'orderBy':\n",
    "            label = self.sql[1][0] # check sql: val_unit1\n",
    "        elif key == 'having':\n",
    "            label = self.sql[2][1][0] # check sql: cond_unit\n",
    "        else: # ADDED\n",
    "            print(\"\\n Unexpected pre-agg key: \", key)\n",
    "            exit()\n",
    "        return self.history, label\n",
    "\n",
    "# TODO: check why not RootTemPredictor\n",
    "\n",
    "# class RootTemPredictor:\n",
    "#     def __init__(self, question, sql):\n",
    "#         self.sql = sql\n",
    "#         self.question = question\n",
    "#         self.keywords = ('intersect', 'except', 'union')\n",
    "#\n",
    "#     def generate_output(self):\n",
    "#         for key in self.sql:\n",
    "#             if key in self.keywords:\n",
    "#                 return ['ROOT'], key, self.sql[key]\n",
    "#         return ['ROOT'], 'none', self.sql\n",
    "\n",
    "\n",
    "# history added orderBy only one col and agg! TODO: CHECK multiple orderBy columns\n",
    "class DesAscPredictor:\n",
    "    def __init__(self, question, sql, table, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "\n",
    "    def generate_output(self):\n",
    "        for key in self.sql: # check sql: whole sql\n",
    "            if key == \"orderBy\" and self.sql[key]:\n",
    "                # self.history.append(key)\n",
    "                try:\n",
    "                    col = self.sql[key][1][0][1][1] # w\n",
    "                except:\n",
    "                    print(\"question:{} sql:{}\".format(self.question, self.sql))\n",
    "                # self.history.append(index_to_column_name(col, self.table))\n",
    "                # self.history.append(self.sql[key][1][0][1][0])\n",
    "                if self.sql[key][0] == \"asc\" and self.sql[\"limit\"]: # TODO: get limit value and labels\n",
    "                    label = 0\n",
    "                elif self.sql[key][0] == \"asc\" and not self.sql[\"limit\"]:\n",
    "                    label = 1\n",
    "                elif self.sql[key][0] == \"desc\" and self.sql[\"limit\"]:\n",
    "                    label = 2\n",
    "                else:\n",
    "                    label = 3                                           # agg_id in col_unit of val_unit in orderBy\n",
    "                return self.history+[index_to_column_name(col, self.table), self.sql[key][1][0][1][0]], label\n",
    "\n",
    "\n",
    "class AndOrPredictor:\n",
    "    def __init__(self, question, sql, table, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "\n",
    "    def generate_output(self):\n",
    "        if 'where' in self.sql and self.sql['where'] and len(self.sql['where']) > 1:\n",
    "            return self.history, COND_OPS[self.sql['where'][1]]\n",
    "        return self.history,-1\n",
    "    \n",
    "    \n",
    "def get_table_dict(table_data_path):\n",
    "    data = json.load(open(table_data_path))\n",
    "    table = dict()\n",
    "    for item in data:\n",
    "        table[item[\"db_id\"]] = item\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_full_history(question_tokens, sql, table, history):\n",
    "    table_schema = [\n",
    "        table[\"table_names\"],\n",
    "        table[\"column_names\"],\n",
    "        table[\"column_types\"]\n",
    "    ]\n",
    "    full_labels = []\n",
    "    masks = [COMPONENTS_DICT['multi_sql']]\n",
    "    stack = [(\"root\", sql)]\n",
    "    with_join = False\n",
    "    fk_dict = defaultdict(list)\n",
    "    for fk in table[\"foreign_keys\"]:\n",
    "        fk_dict[fk[0]].append(fk[1])\n",
    "        fk_dict[fk[1]].append(fk[0])\n",
    "    while len(stack) > 0:\n",
    "        node = stack.pop()\n",
    "        if node[0] == \"root\":\n",
    "            if len(masks) > 1:\n",
    "                masks.append(COMPONENTS_DICT['root_tem'])\n",
    "                full_labels.append(-1)\n",
    "            history, label, sql = MultiSqlPredictor(question_tokens, node[1], history).generate_output()\n",
    "            full_labels.append(SQL_OPS[label])\n",
    "            history.append(label)\n",
    "            if label == \"none\":\n",
    "                stack.append((label, sql))\n",
    "                masks.append(COMPONENTS_DICT['keyword'])\n",
    "            else:\n",
    "                node[1][label] = None\n",
    "                stack.append((label, node[1], sql)) # TODO: double check\n",
    "                masks.append(COMPONENTS_DICT['multi_sql'])\n",
    "                # if label != \"none\":\n",
    "                # stack.append((\"none\",node[1]))\n",
    "        elif node[0] in ('intersect', 'except', 'union'):\n",
    "            stack.append((\"root\", node[1]))\n",
    "            stack.append((\"root\", node[2]))\n",
    "        elif node[0] == \"none\":\n",
    "            with_join = len(node[1][\"from\"][\"table_units\"]) > 1\n",
    "            history, label, sql = KeyWordPredictor(question_tokens, node[1], history).generate_output()\n",
    "            # full_labels.append(label)\n",
    "            # [len(sql_keywords), sql_keywords]\n",
    "\n",
    "            label_idxs = []\n",
    "            for item in label[1]:\n",
    "                if item in KW_DICT:\n",
    "                    label_idxs.append(KW_DICT[item])\n",
    "            label_idxs.sort()\n",
    "            full_labels.append(label_idxs)\n",
    "\n",
    "            if \"orderBy\" in label[1]:\n",
    "                stack.append((\"orderBy\", node[1]))\n",
    "            if \"groupBy\" in label[1]:\n",
    "                has_having = \"having\" in label[1]\n",
    "\n",
    "                stack.append((\"groupBy\", node[1], has_having))\n",
    "            if \"where\" in label[1]:\n",
    "                stack.append((\"where\", node[1]))\n",
    "            if \"select\" in label[1]:\n",
    "                stack.append((\"select\", node[1]))\n",
    "\n",
    "        elif node[0] in (\"select\", \"having\", \"orderBy\"):\n",
    "            # if node[0] != \"orderBy\":\n",
    "            history.append(node[0])\n",
    "            masks.append(COMPONENTS_DICT['col'])\n",
    "\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, node[0]).generate_output()\n",
    "            agg_col_dict = dict()\n",
    "            op_col_dict = dict()\n",
    "            if len(col_ret) > 1:\n",
    "                print(\"\\nWarning: why return more than one col_ret!\")\n",
    "                exit()\n",
    "            # history + [key], (len(cols), cols), sqls\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"\\nWarning: predicted 0 columns!\")\n",
    "                    exit()\n",
    "\n",
    "                full_labels.append(get_label_cols(with_join, fk_dict, l[1]))\n",
    "                for col, sql_item in zip(l[1], s):\n",
    "                    key = \"{}{}{}\".format(col[0][0], col[0][1], col[0][2]) #table_name, column_name, index\n",
    "                    # sql_item: (agg_id, val_unit)/select, val_unit1/orderBy, col_unit1/groupBy, cond_unit/where/having\n",
    "                    if key not in agg_col_dict:\n",
    "                        agg_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        agg_col_dict[key].append((sql_item, col[0])) # for the same col with multiple agg\n",
    "                    if key not in op_col_dict:\n",
    "                        op_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        op_col_dict[key].append((sql_item, col[0]))\n",
    "                for key in agg_col_dict:\n",
    "                    stack.append((\"col\", node[0], agg_col_dict[key], op_col_dict[key]))\n",
    "        elif node[0] == \"col\":\n",
    "            # full_labels.append(node[2][-1])\n",
    "            history.append(node[2][0][1])\n",
    "            if node[1] == \"where\":\n",
    "                # stack.append((\"value\", node[2], \"where\"))\n",
    "                stack.append((\"op\", node[2], \"where\"))\n",
    "                # masks.append(COMPONENTS_DICT['op'])\n",
    "            elif node[1] != \"groupBy\":\n",
    "                labels = []\n",
    "                for sql_item, col in node[2]:\n",
    "                    _, label = AggPredictor(question_tokens, sql_item, history, node[1]).generate_output()\n",
    "                    if label - 1 >= 0:\n",
    "                        labels.append(label - 1) # TODO: check why -1\n",
    "\n",
    "                # print(node[2][0][1][2])\n",
    "                masks.append(COMPONENTS_DICT['agg'])\n",
    "                full_labels.append(labels[:min(len(labels), 3)])\n",
    "\n",
    "                if node[1] == \"having\":\n",
    "                    # stack.append((\"value\", node[2], \"having\"))\n",
    "                    stack.append((\"op\", node[2], \"having\"))\n",
    "                if node[1] == \"orderBy\":\n",
    "                    stack.append((\"des_asc\", sql))\n",
    "\n",
    "                if len(labels) > 0:\n",
    "                    history.append(AGG_OPS[labels[0] + 1])\n",
    "\n",
    "        elif node[0] == \"des_asc\":\n",
    "            orderby_ret = DesAscPredictor(question_tokens, node[1], table, history).generate_output()\n",
    "\n",
    "            if not orderby_ret:\n",
    "                continue\n",
    "            masks.append(COMPONENTS_DICT['des_asc'])\n",
    "            # print(node[1])\n",
    "            history.append(orderby_ret[1])\n",
    "            full_labels.append(orderby_ret[1])\n",
    "            if len(stack) > 0:\n",
    "                masks.append(-1)\n",
    "        elif node[0] == \"value\":\n",
    "            masks.append([COMPONENTS_DICT['value'],COMPONENTS_DICT['root_tem']])\n",
    "            val1 = node[1][3]\n",
    "            val2 = node[1][4]\n",
    "            if val2:\n",
    "                if len(stack) > 0:\n",
    "                    masks.append(-1)\n",
    "                full_labels.append([1,[val1,val2]])\n",
    "                history.append([val1,val2])\n",
    "            else:\n",
    "                if len(stack) > 0:\n",
    "                    masks.append(-1)\n",
    "                full_labels.append([1,[val1]])\n",
    "                history.append([val1])\n",
    "\n",
    "        elif node[0] == \"op\":\n",
    "            labels = []\n",
    "\n",
    "            for sql_item, col in node[1]:\n",
    "                _, label, s = OpPredictor(question_tokens, sql_item, history).generate_output()\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "                    history.append(NEW_WHERE_OPS[label])\n",
    "\n",
    "                # masks.append(COMPONENTS_DICT['root_tem'])\n",
    "                if isinstance(s[0], dict):\n",
    "                    stack.append((\"root\", s[0]))\n",
    "                    masks.append(COMPONENTS_DICT['root_tem'])\n",
    "                    full_labels.append([0,[]])\n",
    "                else:\n",
    "                    stack.append((\"value\",sql_item))\n",
    "\n",
    "            if len(labels) > 2:\n",
    "                print(question_tokens)\n",
    "\n",
    "            masks.append(COMPONENTS_DICT['op'])\n",
    "            full_labels.append(labels)\n",
    "        elif node[0] == \"where\":\n",
    "            history.append(node[0])\n",
    "            hist, andor_label = AndOrPredictor(question_tokens, node[1], table, history).generate_output()\n",
    "\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, \"where\").generate_output()\n",
    "            masks.append(COMPONENTS_DICT['col'])\n",
    "            op_col_dict = dict()\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"Warning: predicted 0 columns!\")\n",
    "                    continue\n",
    "\n",
    "                label = get_label_cols(with_join, fk_dict, l[1])\n",
    "                if len(label) > 1:\n",
    "                    full_labels.append([label,[andor_label]])\n",
    "                    masks.append([COMPONENTS_DICT['col'],COMPONENTS_DICT['andor']])\n",
    "                else:\n",
    "                    full_labels.append([label,[]])\n",
    "                    masks.append([COMPONENTS_DICT['col'], COMPONENTS_DICT['andor']])\n",
    "                # full_labels.append()\n",
    "                for col, sql_item in zip(l[1], s):\n",
    "                    key = \"{}{}{}\".format(col[0][0], col[0][1], col[0][2])\n",
    "                    if key not in op_col_dict:\n",
    "                        op_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        op_col_dict[key].append((sql_item, col[0]))\n",
    "                for key in op_col_dict:\n",
    "                    stack.append((\"col\", \"where\", op_col_dict[key]))\n",
    "        elif node[0] == \"groupBy\":\n",
    "            history.append(node[0])\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, node[0]).generate_output()\n",
    "            masks.append(COMPONENTS_DICT['col'])\n",
    "            # agg_col_dict = dict()\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"Warning: predicted 0 columns!\")\n",
    "                    continue\n",
    "\n",
    "                full_labels.append(get_label_cols(with_join, fk_dict, l[1]))\n",
    "                if node[2]:\n",
    "                    stack.append((\"having\", node[1]))\n",
    "                    full_labels.append(1)\n",
    "                    masks.append(COMPONENTS_DICT['having'])\n",
    "                else:\n",
    "                    full_labels.append(0)\n",
    "                    masks.append(COMPONENTS_DICT['having'])\n",
    "\n",
    "    return history,full_labels,masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_value(conditions,nl,mp):\n",
    "    for cond in conditions:\n",
    "        for i,value in enumerate(cond):\n",
    "            if i < 3:\n",
    "                continue\n",
    "            if not value or isinstance(value,dict):\n",
    "                continue\n",
    "            old_value = value\n",
    "            if isinstance(value,str):\n",
    "                if value[0] in ('\\'','\\\"'):\n",
    "                    value = value[1:-1]\n",
    "                value = value.split()\n",
    "            else:\n",
    "                value = [value]\n",
    "            try:\n",
    "                new_val = 'VALUE_{}'.format(len(mp))\n",
    "                cond[i] = new_val\n",
    "                mp.append(old_value)\n",
    "                if isinstance(value[0],str):\n",
    "                    idx = nl.index(value[0])\n",
    "                else:\n",
    "                    idx = -1\n",
    "                    for i in range(len(nl)):\n",
    "                        if nl[i].isdigit() and (float(nl[i]) == value[0]):\n",
    "                            idx = i\n",
    "                            break\n",
    "                    if idx == -1:\n",
    "                        # print(old_value)\n",
    "                        # print(nl)\n",
    "                        continue\n",
    "                nl = nl[:idx] + [new_val] + nl[idx+len(value):]\n",
    "            except Exception:\n",
    "                # print(old_value)\n",
    "                # print(nl)\n",
    "                continue\n",
    "    return conditions,nl\n",
    "\n",
    "def replace_nl(sql,nl):\n",
    "    mp = []\n",
    "    sql[\"where\"],nl = replace_value(sql[\"where\"],nl,mp)\n",
    "    sql[\"having\"],nl = replace_value(sql[\"having\"],nl,mp)\n",
    "    d = {}\n",
    "    for i,val in enumerate(mp):\n",
    "        d[\"VALUE_{}\".format(i)] = val\n",
    "    return d,sql,nl\n",
    "\n",
    "def parse_data_new_format(data, table_dict):\n",
    "    dataset = []\n",
    "    for item in data:\n",
    "        mp,sql,nl = replace_nl(item[\"sql\"],item[\"question_toks\"])\n",
    "        history,labels,masks = parse_data_full_history(nl, sql, table_dict[item[\"db_id\"]], [])\n",
    "        print('\\n-------------------------------')\n",
    "        dataset.append({\n",
    "            \"history\":history,\n",
    "            \"label\":labels,\n",
    "            \"mask\":masks,\n",
    "            \"map\":mp,\n",
    "            \"nl\":nl\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../SyntaxSQL/data/dev.json'\n",
    "table_data_path = '../../SyntaxSQL/data/tables.json'\n",
    "data = json.load(open(data_path))\n",
    "table_dict = get_table_dict(table_data_path)\n",
    "parse_data_new_format(data, table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "PAD_WORD = '<blank>'\n",
    "PAD = 1\n",
    "BOS_WORD = '<s>'\n",
    "BOS = 2\n",
    "EOS_WORD = '</s>'\n",
    "EOS = 3\n",
    "SKP_WORD = '<sk>'\n",
    "SKP = 4\n",
    "RIG_WORD = '<]>'\n",
    "RIG = 5\n",
    "LFT_WORD = '<[>'\n",
    "LFT = 6\n",
    "special_token_list = [UNK_WORD, PAD_WORD, BOS_WORD, EOS_WORD, SKP_WORD, RIG_WORD, LFT_WORD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_index(tk_list):\n",
    "    stack = [0]\n",
    "    r_list = []\n",
    "    for i, tk in enumerate(tk_list):\n",
    "        r_list.append(stack[-1])\n",
    "        if tk.startswith('('):\n",
    "            # +1: because the parent of the top level is 0\n",
    "            stack.append(i+1)\n",
    "        elif tk ==')':\n",
    "            stack.pop()\n",
    "    # for EOS (</s>)\n",
    "    r_list.append(0)\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def get_tgt_mask(lay_skip):\n",
    "    # 0: use layout encoding vectors; 1: use target word embeddings;\n",
    "    # with a <s> token at the first position\n",
    "    return [1] + [1 if tk in (SKP_WORD, RIG_WORD) else 0 for tk in lay_skip]\n",
    "\n",
    "\n",
    "def get_lay_index(lay_skip):\n",
    "    # with a <s> token at the first position\n",
    "    r_list = [0]\n",
    "    k = 0\n",
    "    for tk in lay_skip:\n",
    "        if tk in (SKP_WORD, RIG_WORD):\n",
    "            r_list.append(0)\n",
    "        else:\n",
    "            r_list.append(k)\n",
    "            k += 1\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def get_tgt_loss(line, mask_target_loss):\n",
    "    r_list = []\n",
    "    for tk_tgt, tk_lay_skip in zip(line['tgt'], line['lay_skip']):\n",
    "        if tk_lay_skip in (SKP_WORD, RIG_WORD):\n",
    "            r_list.append(tk_tgt)\n",
    "        else:\n",
    "            if mask_target_loss:\n",
    "                r_list.append(PAD_WORD)\n",
    "            else:\n",
    "                r_list.append(tk_tgt)\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def __getstate__(self):\n",
    "    return dict(self.__dict__, stoi=dict(self.stoi))\n",
    "\n",
    "\n",
    "def __setstate__(self, state):\n",
    "    self.__dict__.update(state)\n",
    "    self.stoi = defaultdict(lambda: 0, self.stoi)\n",
    "\n",
    "\n",
    "torchtext.vocab.Vocab.__getstate__ = __getstate__\n",
    "torchtext.vocab.Vocab.__setstate__ = __setstate__\n",
    "\n",
    "\n",
    "def filter_counter(freqs, min_freq):\n",
    "    cnt = Counter()\n",
    "    for k, v in freqs.items():\n",
    "        if (min_freq is None) or (v >= min_freq):\n",
    "            cnt[k] = v\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def merge_vocabs(vocabs, min_freq=0, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Merge individual vocabularies (assumed to be generated from disjoint\n",
    "    documents) into a larger vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocabs: `torchtext.vocab.Vocab` vocabularies to be merged\n",
    "        vocab_size: `int` the final vocabulary size. `None` for no limit.\n",
    "    Return:\n",
    "        `torchtext.vocab.Vocab`\n",
    "    \"\"\"\n",
    "    merged = Counter()\n",
    "    for vocab in vocabs:\n",
    "        merged += filter_counter(vocab.freqs, min_freq)\n",
    "    return torchtext.vocab.Vocab(merged,\n",
    "                                 specials=list(special_token_list),\n",
    "                                 max_size=vocab_size, min_freq=min_freq)\n",
    "\n",
    "\n",
    "def join_dicts(*args):\n",
    "    \"\"\"\n",
    "    args: dictionaries with disjoint keys\n",
    "    returns: a single dictionary that has the union of these keys\n",
    "    \"\"\"\n",
    "    return dict(chain(*[d.items() for d in args]))\n",
    "\n",
    "def _preprocess_json(js):\n",
    "    t = SCode((js['token'], js['type']))\n",
    "    js['lay'] = t.layout(add_skip=False)\n",
    "    js['lay_skip'] = t.layout(add_skip=True)\n",
    "    assert len(t.target()) == len(js['lay_skip']), (list(zip(t.target(), js['lay_skip'])), ' '.join(js['tgt']))\n",
    "    js['tgt'] = t.target()\n",
    "\n",
    "def read_anno_json(anno_path):\n",
    "    with codecs.open(anno_path, \"r\", \"utf-8\") as corpus_file:\n",
    "        js_list = [json.loads(line) for line in corpus_file]\n",
    "        js_list = js_list[:5]\n",
    "        for js in js_list:\n",
    "            _preprocess_json(js)\n",
    "    return js_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_list = read_anno_json(test_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(js_list):\n",
    "    for js in js_list:\n",
    "        print(\"\\n\"+\"-\"*50)\n",
    "        for field in ['src', \"token\", \"type\", 'lay', \"lay_index\", \"lay_parent_index\",\\\n",
    "                      \"copy_to_tgt\", \"copy_to_ext\", \"tgt_mask\", \"tgt\", \"tgt_copy_ext\",\\\n",
    "                      \"tgt_parent_index\", \"tgt_loss\"]:\n",
    "            if field in ('src', 'lay', \"token\", \"type\"):\n",
    "                lines = js[field]\n",
    "            elif field in ('copy_to_tgt','copy_to_ext'):\n",
    "                lines = js['src']\n",
    "            elif field in ('tgt',):\n",
    "                def _tgt(line):\n",
    "                    r_list = []\n",
    "                    for tk_tgt, tk_lay_skip in zip(line['tgt'], line['lay_skip']):\n",
    "                        if tk_lay_skip in (SKP_WORD, RIG_WORD):\n",
    "                            r_list.append(tk_tgt)\n",
    "                        else:\n",
    "                            r_list.append(PAD_WORD)\n",
    "                    return r_list\n",
    "                lines = _tgt(js)\n",
    "            elif field in ('tgt_copy_ext',):\n",
    "                def _tgt_copy_ext(line):\n",
    "                    r_list = []\n",
    "                    src_set = set(line['src'])\n",
    "                    for tk_tgt in line['tgt']:\n",
    "                        if tk_tgt in src_set:\n",
    "                            r_list.append(tk_tgt)\n",
    "                        else:\n",
    "                            r_list.append(UNK_WORD)\n",
    "                    return r_list\n",
    "                lines = _tgt_copy_ext(js)\n",
    "            elif field in ('tgt_loss',):\n",
    "                lines = get_tgt_loss(js, False)\n",
    "            elif field in ('tgt_mask',):\n",
    "                lines = get_tgt_mask(js['lay_skip'])\n",
    "            elif field in ('lay_index',):\n",
    "                lines = get_lay_index(js['lay_skip'])\n",
    "            elif field in ('lay_parent_index',):\n",
    "                lines = get_parent_index(js['lay'])\n",
    "            elif field in ('tgt_parent_index',):\n",
    "                lines = get_parent_index(js['tgt'])\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            print(field + \": \", lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "src:  ['send', 'a', 'signal', '`', 'signal.SIGUSR1', '`', 'to', 'the', 'current', 'process']\n",
      "token:  ['os', '.', 'kill', '(', 'os', '.', 'getpid', '(', ')', ',', 'signal', '.', 'SIGUSR1', ')']\n",
      "type:  ['KEYWORD', 'OP', 'KEYWORD', 'OP', 'KEYWORD', 'OP', 'KEYWORD', 'OP', 'OP', 'OP', 'KEYWORD', 'OP', 'KEYWORD', 'OP']\n",
      "lay:  ['os', '.', 'kill', '(', 'os', '.', 'getpid', '(', ')', ',', 'signal', '.', 'SIGUSR1', ')']\n",
      "lay_index:  [0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "lay_parent_index:  [0, 0, 0, 0, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 0]\n",
      "copy_to_tgt:  ['send', 'a', 'signal', '`', 'signal.SIGUSR1', '`', 'to', 'the', 'current', 'process']\n",
      "copy_to_ext:  ['send', 'a', 'signal', '`', 'signal.SIGUSR1', '`', 'to', 'the', 'current', 'process']\n",
      "tgt_mask:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tgt:  ['<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>']\n",
      "tgt_copy_ext:  ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'signal', '<unk>', '<unk>', '<unk>']\n",
      "tgt_parent_index:  [0, 0, 0, 0, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 0]\n",
      "tgt_loss:  ['os', '.', 'kill', '(', 'os', '.', 'getpid', '(', ')', ',', 'signal', '.', 'SIGUSR1', ')']\n",
      "\n",
      "--------------------------------------------------\n",
      "src:  ['decode', 'a', 'hex', 'string', \"'4a4b4c\", \"'\", 'to', 'UTF-8', '.']\n",
      "token:  ['bytes', '.', 'fromhex', '(', \"'4a4b4c'\", ')', '.', 'decode', '(', \"'utf-8'\", ')']\n",
      "type:  ['KEYWORD', 'OP', 'KEYWORD', 'OP', 'STRING', 'OP', 'OP', 'KEYWORD', 'OP', 'STRING', 'OP']\n",
      "lay:  ['bytes', '.', 'fromhex', '(', 'STRING', ')', '.', 'decode', '(', 'STRING', ')']\n",
      "lay_index:  [0, 0, 1, 2, 3, 4, 0, 5, 6, 7, 8, 9, 0, 10]\n",
      "lay_parent_index:  [0, 0, 0, 0, 4, 4, 0, 0, 0, 9, 9, 0]\n",
      "copy_to_tgt:  ['decode', 'a', 'hex', 'string', \"'4a4b4c\", \"'\", 'to', 'UTF-8', '.']\n",
      "copy_to_ext:  ['decode', 'a', 'hex', 'string', \"'4a4b4c\", \"'\", 'to', 'UTF-8', '.']\n",
      "tgt_mask:  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "tgt:  ['<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>']\n",
      "tgt_copy_ext:  ['<unk>', '.', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.', 'decode', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "tgt_parent_index:  [0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 10, 10, 10, 0]\n",
      "tgt_loss:  ['bytes', '.', 'fromhex', '(', '<[>', '<]>', ')', '.', 'decode', '(', '<[>', '<]>', ')']\n",
      "\n",
      "--------------------------------------------------\n",
      "src:  ['check', 'if', 'all', 'elements', 'in', 'list', '`', 'myList', '`', 'are', 'identical']\n",
      "token:  ['all', '(', 'x', '==', 'myList', '[', '0', ']', 'for', 'x', 'in', 'myList', ')']\n",
      "type:  ['KEYWORD', 'OP', 'NAME', 'OP', 'KEYWORD', 'OP', 'NUMBER', 'OP', 'KEYWORD', 'NAME', 'KEYWORD', 'KEYWORD', 'OP']\n",
      "lay:  ['all', '(', 'NAME', '==', 'myList', '[', 'NUMBER', ']', 'for', 'NAME', 'in', 'myList', ')']\n",
      "lay_index:  [0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "lay_parent_index:  [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n",
      "copy_to_tgt:  ['check', 'if', 'all', 'elements', 'in', 'list', '`', 'myList', '`', 'are', 'identical']\n",
      "copy_to_ext:  ['check', 'if', 'all', 'elements', 'in', 'list', '`', 'myList', '`', 'are', 'identical']\n",
      "tgt_mask:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tgt:  ['<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>']\n",
      "tgt_copy_ext:  ['all', '<unk>', '<unk>', '<unk>', 'myList', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'in', 'myList', '<unk>']\n",
      "tgt_parent_index:  [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n",
      "tgt_loss:  ['all', '(', 'x', '==', 'myList', '[', '0', ']', 'for', 'x', 'in', 'myList', ')']\n",
      "\n",
      "--------------------------------------------------\n",
      "src:  ['format', 'number', 'of', 'spaces', 'between', 'strings', '`', 'Python', '`', ',', '`', ':', '`', 'and', '`', 'Very', 'Good', '`', 'to', 'be', '`', '20', '`']\n",
      "token:  ['print', '(', \"'%*s#SPACE#:#SPACE#%*s'\", '%', '(', '20', ',', \"'Python'\", ',', '20', ',', \"'Very#SPACE#Good'\", ')', ')']\n",
      "type:  ['KEYWORD', 'OP', 'STRING', 'OP', 'OP', 'NUMBER', 'OP', 'STRING', 'OP', 'NUMBER', 'OP', 'STRING', 'OP', 'OP']\n",
      "lay:  ['print', '(', 'STRING', '%', '(', 'NUMBER', ',', 'STRING', ',', 'NUMBER', ',', 'STRING', ')', ')']\n",
      "lay_index:  [0, 0, 1, 2, 0, 3, 4, 5, 6, 7, 0, 8, 9, 10, 11, 0, 12, 13]\n",
      "lay_parent_index:  [0, 0, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0]\n",
      "copy_to_tgt:  ['format', 'number', 'of', 'spaces', 'between', 'strings', '`', 'Python', '`', ',', '`', ':', '`', 'and', '`', 'Very', 'Good', '`', 'to', 'be', '`', '20', '`']\n",
      "copy_to_ext:  ['format', 'number', 'of', 'spaces', 'between', 'strings', '`', 'Python', '`', ',', '`', ':', '`', 'and', '`', 'Very', 'Good', '`', 'to', 'be', '`', '20', '`']\n",
      "tgt_mask:  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "tgt:  ['<blank>', '<blank>', '<blank>', '<]>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>', '<blank>']\n",
      "tgt_copy_ext:  ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '20', ',', '<unk>', '<unk>', ',', '20', ',', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "tgt_parent_index:  [0, 0, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 0]\n",
      "tgt_loss:  ['print', '(', '<[>', '<]>', '%', '(', '20', ',', '<[>', '<]>', ',', '20', ',', '<[>', '<]>', ')', ')']\n",
      "\n",
      "--------------------------------------------------\n",
      "src:  ['How', 'to', 'convert', 'a', 'string', 'from', 'CP-1251', 'to', 'UTF-8', '?']\n",
      "token:  ['d', '.', 'decode', '(', \"'cp1251'\", ')', '.', 'encode', '(', \"'utf8'\", ')']\n",
      "type:  ['NAME', 'OP', 'KEYWORD', 'OP', 'STRING', 'OP', 'OP', 'KEYWORD', 'OP', 'STRING', 'OP']\n",
      "lay:  ['NAME', '.', 'decode', '(', 'STRING', ')', '.', 'encode', '(', 'STRING', ')']\n",
      "lay_index:  [0, 0, 1, 2, 3, 4, 0, 5, 6, 7, 8, 9, 0, 10]\n",
      "lay_parent_index:  [0, 0, 0, 0, 4, 4, 0, 0, 0, 9, 9, 0]\n",
      "copy_to_tgt:  ['How', 'to', 'convert', 'a', 'string', 'from', 'CP-1251', 'to', 'UTF-8', '?']\n",
      "copy_to_ext:  ['How', 'to', 'convert', 'a', 'string', 'from', 'CP-1251', 'to', 'UTF-8', '?']\n",
      "tgt_mask:  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "tgt:  ['<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>', '<blank>', '<blank>', '<blank>', '<blank>', '<]>', '<blank>']\n",
      "tgt_copy_ext:  ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "tgt_parent_index:  [0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 10, 10, 10, 0]\n",
      "tgt_loss:  ['d', '.', 'decode', '(', '<[>', '<]>', ')', '.', 'encode', '(', '<[>', '<]>', ')']\n"
     ]
    }
   ],
   "source": [
    "process_data(js_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
