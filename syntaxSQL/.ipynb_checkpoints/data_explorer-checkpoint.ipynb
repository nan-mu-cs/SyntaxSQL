{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, count\n",
    "from six import string_types\n",
    "import torch\n",
    "import torchtext.data\n",
    "import torchtext.vocab\n",
    "\n",
    "import table\n",
    "import table.IO\n",
    "import opts\n",
    "from tree import SCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "PAD_WORD = '<blank>'\n",
    "PAD = 1\n",
    "BOS_WORD = '<s>'\n",
    "BOS = 2\n",
    "EOS_WORD = '</s>'\n",
    "EOS = 3\n",
    "SKP_WORD = '<sk>'\n",
    "SKP = 4\n",
    "RIG_WORD = '<]>'\n",
    "RIG = 5\n",
    "LFT_WORD = '<[>'\n",
    "LFT = 6\n",
    "special_token_list = [UNK_WORD, PAD_WORD, BOS_WORD, EOS_WORD, SKP_WORD, RIG_WORD, LFT_WORD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_index(tk_list):\n",
    "    stack = [0]\n",
    "    r_list = []\n",
    "    for i, tk in enumerate(tk_list):\n",
    "        r_list.append(stack[-1])\n",
    "        if tk.startswith('('):\n",
    "            # +1: because the parent of the top level is 0\n",
    "            stack.append(i+1)\n",
    "        elif tk ==')':\n",
    "            stack.pop()\n",
    "    # for EOS (</s>)\n",
    "    r_list.append(0)\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def get_tgt_mask(lay_skip):\n",
    "    # 0: use layout encoding vectors; 1: use target word embeddings;\n",
    "    # with a <s> token at the first position\n",
    "    return [1] + [1 if tk in (SKP_WORD, RIG_WORD) else 0 for tk in lay_skip]\n",
    "\n",
    "\n",
    "def get_lay_index(lay_skip):\n",
    "    # with a <s> token at the first position\n",
    "    r_list = [0]\n",
    "    k = 0\n",
    "    for tk in lay_skip:\n",
    "        if tk in (SKP_WORD, RIG_WORD):\n",
    "            r_list.append(0)\n",
    "        else:\n",
    "            r_list.append(k)\n",
    "            k += 1\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def get_tgt_loss(line, mask_target_loss):\n",
    "    r_list = []\n",
    "    for tk_tgt, tk_lay_skip in zip(line['tgt'], line['lay_skip']):\n",
    "        if tk_lay_skip in (SKP_WORD, RIG_WORD):\n",
    "            r_list.append(tk_tgt)\n",
    "        else:\n",
    "            if mask_target_loss:\n",
    "                r_list.append(PAD_WORD)\n",
    "            else:\n",
    "                r_list.append(tk_tgt)\n",
    "    return r_list\n",
    "\n",
    "\n",
    "def __getstate__(self):\n",
    "    return dict(self.__dict__, stoi=dict(self.stoi))\n",
    "\n",
    "\n",
    "def __setstate__(self, state):\n",
    "    self.__dict__.update(state)\n",
    "    self.stoi = defaultdict(lambda: 0, self.stoi)\n",
    "\n",
    "\n",
    "torchtext.vocab.Vocab.__getstate__ = __getstate__\n",
    "torchtext.vocab.Vocab.__setstate__ = __setstate__\n",
    "\n",
    "\n",
    "def filter_counter(freqs, min_freq):\n",
    "    cnt = Counter()\n",
    "    for k, v in freqs.items():\n",
    "        if (min_freq is None) or (v >= min_freq):\n",
    "            cnt[k] = v\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def merge_vocabs(vocabs, min_freq=0, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Merge individual vocabularies (assumed to be generated from disjoint\n",
    "    documents) into a larger vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocabs: `torchtext.vocab.Vocab` vocabularies to be merged\n",
    "        vocab_size: `int` the final vocabulary size. `None` for no limit.\n",
    "    Return:\n",
    "        `torchtext.vocab.Vocab`\n",
    "    \"\"\"\n",
    "    merged = Counter()\n",
    "    for vocab in vocabs:\n",
    "        merged += filter_counter(vocab.freqs, min_freq)\n",
    "    return torchtext.vocab.Vocab(merged,\n",
    "                                 specials=list(special_token_list),\n",
    "                                 max_size=vocab_size, min_freq=min_freq)\n",
    "\n",
    "\n",
    "def join_dicts(*args):\n",
    "    \"\"\"\n",
    "    args: dictionaries with disjoint keys\n",
    "    returns: a single dictionary that has the union of these keys\n",
    "    \"\"\"\n",
    "    return dict(chain(*[d.items() for d in args]))\n",
    "\n",
    "def _preprocess_json(js, opt):\n",
    "    t = SCode((js['token'], js['type']))\n",
    "    js['lay'] = t.layout(add_skip=False)\n",
    "    js['lay_skip'] = t.layout(add_skip=True)\n",
    "    assert len(t.target()) == len(js['lay_skip']), (list(zip(t.target(), js['lay_skip'])), ' '.join(js['tgt']))\n",
    "    js['tgt'] = t.target()\n",
    "\n",
    "def read_anno_json(anno_path, opt):\n",
    "    with codecs.open(anno_path, \"r\", \"utf-8\") as corpus_file:\n",
    "        js_list = [json.loads(line) for line in corpus_file]\n",
    "        js_list = js_list[:5]\n",
    "        for js in js_list:\n",
    "            _preprocess_json(js, opt)\n",
    "    return js_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(torchtext.data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        \"Sort in reverse size order\"\n",
    "        return -len(ex.src)\n",
    "\n",
    "    def __init__(self, anno, fields, permute_order, opt, filter_ex, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a TranslationDataset given paths and fields.\n",
    "\n",
    "        anno: location of annotated data / js_list\n",
    "        filter_ex: False - keep all the examples for evaluation (should not have filtered examples); True - filter examples with unmatched spans;\n",
    "        \"\"\"\n",
    "        if isinstance(anno, string_types):\n",
    "            js_list = read_anno_json(anno, opt)\n",
    "        else:\n",
    "            js_list = anno\n",
    "\n",
    "        src_data = self._read_annotated_file(opt, js_list, 'src', filter_ex)\n",
    "        src_examples = self._construct_examples(src_data, 'src')\n",
    "\n",
    "        lay_data = self._read_annotated_file(opt, js_list, 'lay', filter_ex)\n",
    "        lay_examples = self._construct_examples(lay_data, 'lay')\n",
    "\n",
    "        # without <s> and </s>\n",
    "        lay_e_data = self._read_annotated_file(opt, js_list, 'lay', filter_ex)\n",
    "        lay_e_examples = self._construct_examples(lay_e_data, 'lay_e')\n",
    "\n",
    "        lay_index_data = self._read_annotated_file(\n",
    "            opt, js_list, 'lay_index', filter_ex)\n",
    "        lay_index_examples = self._construct_examples(\n",
    "            lay_index_data, 'lay_index')\n",
    "\n",
    "        lay_parent_index_data = self._read_annotated_file(\n",
    "            opt, js_list, 'lay_parent_index', filter_ex)\n",
    "        lay_parent_index_examples = self._construct_examples(\n",
    "            lay_parent_index_data, 'lay_parent_index')\n",
    "\n",
    "        copy_to_tgt_data = self._read_annotated_file(\n",
    "            opt, js_list, 'copy_to_tgt', filter_ex)\n",
    "        copy_to_tgt_examples = self._construct_examples(\n",
    "            copy_to_tgt_data, 'copy_to_tgt')\n",
    "\n",
    "        copy_to_ext_data = self._read_annotated_file(\n",
    "            opt, js_list, 'copy_to_ext', filter_ex)\n",
    "        copy_to_ext_examples = self._construct_examples(\n",
    "            copy_to_ext_data, 'copy_to_ext')\n",
    "\n",
    "        tgt_mask_data = self._read_annotated_file(\n",
    "            opt, js_list, 'tgt_mask', filter_ex)\n",
    "        tgt_mask_examples = self._construct_examples(tgt_mask_data, 'tgt_mask')\n",
    "\n",
    "        tgt_data = self._read_annotated_file(opt, js_list, 'tgt', filter_ex)\n",
    "        tgt_examples = self._construct_examples(tgt_data, 'tgt')\n",
    "\n",
    "        tgt_parent_index_data = self._read_annotated_file(\n",
    "            opt, js_list, 'tgt_parent_index', filter_ex)\n",
    "        tgt_parent_index_examples = self._construct_examples(\n",
    "            tgt_parent_index_data, 'tgt_parent_index')\n",
    "\n",
    "        tgt_loss_data = self._read_annotated_file(\n",
    "            opt, js_list, 'tgt_loss', filter_ex)\n",
    "        tgt_loss_examples = self._construct_examples(tgt_loss_data, 'tgt_loss')\n",
    "\n",
    "        tgt_copy_ext_data = self._read_annotated_file(\n",
    "            opt, js_list, 'tgt_copy_ext', filter_ex)\n",
    "        tgt_copy_ext_examples = self._construct_examples(tgt_copy_ext_data, 'tgt_copy_ext')\n",
    "\n",
    "        # examples: one for each src line or (src, tgt) line pair.\n",
    "        examples = [join_dicts(*it) for it in zip(src_examples, lay_examples, lay_e_examples, lay_index_examples, lay_parent_index_examples, copy_to_tgt_examples, copy_to_ext_examples, tgt_mask_examples, tgt_examples, tgt_parent_index_examples, tgt_loss_examples, tgt_copy_ext_examples)]\n",
    "        # the examples should not contain None\n",
    "        len_before_filter = len(examples)\n",
    "        examples = list(filter(lambda x: all(\n",
    "            (v is not None for k, v in x.items())), examples))\n",
    "        len_after_filter = len(examples)\n",
    "        num_filter = len_before_filter - len_after_filter\n",
    "        if num_filter > 0:\n",
    "            print('Filter #examples (with None): {} / {} = {:.2%}'.format(num_filter,\n",
    "                                                                          len_before_filter, num_filter / len_before_filter))\n",
    "\n",
    "        # Peek at the first to see which fields are used.\n",
    "        ex = examples[0]\n",
    "        keys = ex.keys()\n",
    "        fields = [(k, fields[k])\n",
    "                  for k in (list(keys) + [\"indices\"])]\n",
    "\n",
    "        def construct_final(examples):\n",
    "            for i, ex in enumerate(examples):\n",
    "                yield torchtext.data.Example.fromlist(\n",
    "                    [ex[k] for k in keys] + [i],\n",
    "                    fields)\n",
    "\n",
    "        def filter_pred(example):\n",
    "            return True\n",
    "\n",
    "        super(TableDataset, self).__init__(\n",
    "            construct_final(examples), fields, filter_pred)\n",
    "\n",
    "    def _read_annotated_file(self, opt, js_list, field, filter_ex):\n",
    "        \"\"\"\n",
    "        path: location of a src or tgt file\n",
    "        truncate: maximum sequence length (0 for unlimited)\n",
    "        \"\"\"\n",
    "        if field in ('src', 'lay'):\n",
    "            lines = (line[field] for line in js_list)\n",
    "        elif field in ('copy_to_tgt','copy_to_ext'):\n",
    "            lines = (line['src'] for line in js_list)\n",
    "        elif field in ('tgt',):\n",
    "            def _tgt(line):\n",
    "                r_list = []\n",
    "                for tk_tgt, tk_lay_skip in zip(line['tgt'], line['lay_skip']):\n",
    "                    if tk_lay_skip in (SKP_WORD, RIG_WORD):\n",
    "                        r_list.append(tk_tgt)\n",
    "                    else:\n",
    "                        r_list.append(PAD_WORD)\n",
    "                return r_list\n",
    "            lines = (_tgt(line) for line in js_list)\n",
    "        elif field in ('tgt_copy_ext',):\n",
    "            def _tgt_copy_ext(line):\n",
    "                r_list = []\n",
    "                src_set = set(line['src'])\n",
    "                for tk_tgt in line['tgt']:\n",
    "                    if tk_tgt in src_set:\n",
    "                        r_list.append(tk_tgt)\n",
    "                    else:\n",
    "                        r_list.append(UNK_WORD)\n",
    "                return r_list\n",
    "            lines = (_tgt_copy_ext(line) for line in js_list)\n",
    "        elif field in ('tgt_loss',):\n",
    "            lines = (get_tgt_loss(line, False) for line in js_list)\n",
    "        elif field in ('tgt_mask',):\n",
    "            lines = (get_tgt_mask(line['lay_skip']) for line in js_list)\n",
    "        elif field in ('lay_index',):\n",
    "            lines = (get_lay_index(line['lay_skip']) for line in js_list)\n",
    "        elif field in ('lay_parent_index',):\n",
    "            lines = (get_parent_index(line['lay']) for line in js_list)\n",
    "        elif field in ('tgt_parent_index',):\n",
    "            lines = (get_parent_index(line['tgt']) for line in js_list)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for line in lines:\n",
    "            yield line\n",
    "\n",
    "    def _construct_examples(self, lines, side):\n",
    "        for words in lines:\n",
    "            example_dict = {side: words}\n",
    "            yield example_dict\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def __reduce_ex__(self, proto):\n",
    "        \"This is a hack. Something is broken with torch pickle.\"\n",
    "        return super(TableDataset, self).__reduce_ex__()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_fields(vocab):\n",
    "        vocab = dict(vocab)\n",
    "        fields = TableDataset.get_fields()\n",
    "        for k, v in vocab.items():\n",
    "            # Hack. Can't pickle defaultdict :(\n",
    "            v.stoi = defaultdict(lambda: 0, v.stoi)\n",
    "            fields[k].vocab = v\n",
    "        return fields\n",
    "\n",
    "    @staticmethod\n",
    "    def save_vocab(fields):\n",
    "        vocab = []\n",
    "        for k, f in fields.items():\n",
    "            if 'vocab' in f.__dict__:\n",
    "                f.vocab.stoi = dict(f.vocab.stoi)\n",
    "                vocab.append((k, f.vocab))\n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fields():\n",
    "        fields = {}\n",
    "        fields[\"src\"] = torchtext.data.Field(\n",
    "            pad_token=PAD_WORD, include_lengths=True)\n",
    "        fields[\"lay\"] = torchtext.data.Field(\n",
    "            init_token=BOS_WORD, include_lengths=True, eos_token=EOS_WORD, pad_token=PAD_WORD)\n",
    "        fields[\"lay_e\"] = torchtext.data.Field(\n",
    "            include_lengths=False, pad_token=PAD_WORD)\n",
    "        fields[\"lay_index\"] = torchtext.data.Field(\n",
    "            use_vocab=False, pad_token=0)\n",
    "        fields[\"lay_parent_index\"] = torchtext.data.Field(\n",
    "            use_vocab=False, pad_token=0)\n",
    "        fields[\"copy_to_tgt\"] = torchtext.data.Field(pad_token=UNK_WORD)\n",
    "        fields[\"copy_to_ext\"] = torchtext.data.Field(pad_token=UNK_WORD)\n",
    "        fields[\"tgt_mask\"] = torchtext.data.Field(\n",
    "            use_vocab=False, dtype=torch.FloatTensor, pad_token=1)\n",
    "        fields[\"tgt\"] = torchtext.data.Field(\n",
    "            init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=PAD_WORD)\n",
    "        fields[\"tgt_copy_ext\"] = torchtext.data.Field(\n",
    "            init_token=UNK_WORD, eos_token=UNK_WORD, pad_token=UNK_WORD)\n",
    "        fields[\"tgt_parent_index\"] = torchtext.data.Field(\n",
    "            use_vocab=False, pad_token=0)\n",
    "        fields[\"tgt_loss\"] = torchtext.data.Field(\n",
    "            init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=PAD_WORD)\n",
    "        fields[\"indices\"] = torchtext.data.Field(\n",
    "            use_vocab=False, sequential=False)\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anno = os.path.join('../data_model/django/test.json')\n",
    "fields = TableDataset.get_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = table.IO.TableDataset(test_anno, fields, 0, opt, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
