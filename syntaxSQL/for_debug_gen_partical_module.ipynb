{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, count\n",
    "from six import string_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
    "NEW_WHERE_OPS = ('=','>','<','>=','<=','!=','like','not in','in','between','is')\n",
    "NEW_WHERE_DICT = {\n",
    "    '=': 0,\n",
    "    '>': 1,\n",
    "    '<': 2,\n",
    "    '>=': 3,\n",
    "    '<=': 4,\n",
    "    '!=': 5,\n",
    "    'like': 6,\n",
    "    'not in': 7,\n",
    "    'in': 8,\n",
    "    'between': 9,\n",
    "    'is':10\n",
    "}\n",
    "# SQL_OPS = ('none','intersect', 'union', 'except')\n",
    "SQL_OPS = {\n",
    "    'none': 0,\n",
    "    'intersect': 1,\n",
    "    'union': 2,\n",
    "    'except': 3\n",
    "}\n",
    "KW_DICT = {\n",
    "    'where': 0,\n",
    "    'groupBy': 1,\n",
    "    'orderBy': 2\n",
    "}\n",
    "ORDER_OPS = {\n",
    "    'desc': 0,\n",
    "    'asc': 1}\n",
    "AGG_OPS = ('none','max', 'min', 'count', 'sum', 'avg')\n",
    "\n",
    "COND_OPS = {\n",
    "    'and':0,\n",
    "    'or':1\n",
    "}\n",
    "\n",
    "ORDER_DICT = {\n",
    "    0:\"asc_limit\",\n",
    "    1:\"asc\",\n",
    "    2:\"desc_limit\",\n",
    "    3: \"desc\"\n",
    "}\n",
    "\n",
    "TRAIN_COMPONENTS = ('multi_sql','keyword','col','op','agg','root_tem','des_asc','having','andor','value')\n",
    "COMPONENTS_DICT = {\n",
    "    'multi_sql':0,\n",
    "    'keyword':1,\n",
    "    'col':2,\n",
    "    'op':3,\n",
    "    'agg':4,\n",
    "    'root_tem':5,\n",
    "    'des_asc':6,\n",
    "    'having':7,\n",
    "    'andor':8,\n",
    "    'value':9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_op_index(is_not,op):\n",
    "    op = OLD_WHERE_OPS[op]\n",
    "    if is_not and op == \"in\":\n",
    "        return 7\n",
    "    try:\n",
    "        return NEW_WHERE_DICT[op]\n",
    "    except:\n",
    "        print(\"Unsupport op: {}\".format(op)) # TODO: check ! =\n",
    "        return -1\n",
    "\n",
    "def index_to_column_name(index, table):\n",
    "    column_name = table[\"column_names\"][index][1]\n",
    "    table_index = table[\"column_names\"][index][0]\n",
    "    table_name = table[\"table_names\"][table_index]\n",
    "    return table_name, column_name, index\n",
    "\n",
    "\n",
    "def get_label_cols(with_join,fk_dict,labels):\n",
    "    # list(set([l[1][i][0][2] for i in range(min(len(l[1]), 3))]))\n",
    "    cols = set()\n",
    "    ret = []\n",
    "    for i in range(len(labels)):\n",
    "        cols.add(labels[i][0][2]) # still col index\n",
    "        if len(cols) > 3:\n",
    "            break\n",
    "    for col in cols:\n",
    "        # ret.append([col])\n",
    "        if with_join and len(fk_dict[col]) > 0:\n",
    "            ret.append(col) #ret.append([col]+fk_dict[col]) TODO: fk_dict removed\n",
    "        else:\n",
    "            ret.append(col)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# history added\n",
    "class MultiSqlPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.keywords = ('intersect', 'except', 'union')\n",
    "\n",
    "    def generate_output(self):\n",
    "        for key in self.sql:\n",
    "            if key in self.keywords and self.sql[key]:\n",
    "                return self.history + ['root'], key, self.sql[key]\n",
    "        return self.history + ['root'], 'none', self.sql\n",
    "\n",
    "\n",
    "class KeyWordPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'limit', 'having')\n",
    "\n",
    "    def generate_output(self):\n",
    "        sql_keywords = []\n",
    "        for key in self.sql:\n",
    "            if key in self.keywords and self.sql[key]: # included other keywords\n",
    "                sql_keywords.append(key)\n",
    "        return self.history, [len(sql_keywords), sql_keywords], self.sql\n",
    "\n",
    "\n",
    "# history added\n",
    "class ColPredictor:\n",
    "    def __init__(self, question, sql, table, history, kw=None):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "        self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'having')\n",
    "        self.kw = kw\n",
    "\n",
    "    def generate_output(self):\n",
    "        ret = []\n",
    "        candidate_keys = self.sql.keys()\n",
    "        if self.kw:\n",
    "            candidate_keys = [self.kw]\n",
    "        for key in candidate_keys:\n",
    "            if key in self.keywords and self.sql[key]:\n",
    "                cols = []\n",
    "                sqls = []\n",
    "                if key == 'groupBy':\n",
    "                    sql_cols = self.sql[key]\n",
    "                    for col in sql_cols:\n",
    "                        cols.append((index_to_column_name(col[1], self.table), col[2]))\n",
    "                        sqls.append(col) # col_unit1\n",
    "                elif key == 'orderBy':\n",
    "                    sql_cols = self.sql[key][1]\n",
    "                    for col in sql_cols: # only contain col_unit1 in val_unit: (unit_op, col_unit1, col_unit2)\n",
    "                        cols.append((index_to_column_name(col[1][1], self.table), col[1][2]))\n",
    "                        sqls.append(col) # val_unit1\n",
    "                elif key == 'select':\n",
    "                    sql_cols = self.sql[key][1]\n",
    "                    for col in sql_cols:  # only contain col_unit1 in val_unit\n",
    "                        cols.append((index_to_column_name(col[1][1][1], self.table), col[1][1][2]))\n",
    "                        sqls.append(col) # (agg_id, val_unit)\n",
    "                elif key == 'where' or key == 'having':\n",
    "                    sql_cols = self.sql[key]\n",
    "                    for col in sql_cols: # TODO: check this one!\n",
    "                        if not isinstance(col, list):\n",
    "                            continue\n",
    "                        try: # col_id of col_unit of val_unit of cond_unit of condition\n",
    "                            cols.append((index_to_column_name(col[2][1][1], self.table), col[2][1][2]))\n",
    "                        except:\n",
    "                            print(\"Key:{} Col:{} Question:{}\".format(key, col, self.question))\n",
    "                        sqls.append(col) # cond_unit\n",
    "                ret.append((\n",
    "                    self.history + [key], (len(cols), cols), sqls\n",
    "                ))\n",
    "        return ret\n",
    "        # ret.append(history+[key],)\n",
    "\n",
    "\n",
    "class OpPredictor:\n",
    "    def __init__(self, question, sql, history):\n",
    "        self.sql = sql # check sql is cond_unit\n",
    "        self.question = question\n",
    "        self.history = history # history not change\n",
    "        # self.keywords = ('select', 'where', 'groupBy', 'orderBy', 'having')\n",
    "\n",
    "    def generate_output(self): # sql3: val_unit, sql4: val1\n",
    "        return self.history, convert_to_op_index(self.sql[0],self.sql[1]), (self.sql[3], self.sql[4])\n",
    "\n",
    "\n",
    "class AggPredictor:\n",
    "    def __init__(self, question, sql, history,kw=None):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.kw = kw\n",
    "    def generate_output(self):\n",
    "        label = -1\n",
    "        if self.kw:\n",
    "            key = self.kw\n",
    "        else:\n",
    "            key = self.history[-2]\n",
    "        if key == 'select':\n",
    "            label = self.sql[0] # check sql: (agg_id, val_unit)\n",
    "        elif key == 'orderBy':\n",
    "            label = self.sql[1][0] # check sql: val_unit1\n",
    "        elif key == 'having':\n",
    "            label = self.sql[2][1][0] # check sql: cond_unit\n",
    "        else: # ADDED\n",
    "            print(\"\\n Unexpected pre-agg key: \", key)\n",
    "            exit()\n",
    "        return self.history, label\n",
    "\n",
    "# TODO: check why not RootTemPredictor\n",
    "\n",
    "# class RootTemPredictor:\n",
    "#     def __init__(self, question, sql):\n",
    "#         self.sql = sql\n",
    "#         self.question = question\n",
    "#         self.keywords = ('intersect', 'except', 'union')\n",
    "#\n",
    "#     def generate_output(self):\n",
    "#         for key in self.sql:\n",
    "#             if key in self.keywords:\n",
    "#                 return ['ROOT'], key, self.sql[key]\n",
    "#         return ['ROOT'], 'none', self.sql\n",
    "\n",
    "\n",
    "# history added orderBy only one col and agg! TODO: CHECK multiple orderBy columns\n",
    "class DesAscPredictor:\n",
    "    def __init__(self, question, sql, table, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "\n",
    "    def generate_output(self):\n",
    "        for key in self.sql: # check sql: whole sql\n",
    "            if key == \"orderBy\" and self.sql[key]:\n",
    "                # self.history.append(key)\n",
    "                try:\n",
    "                    col = self.sql[key][1][0][1][1] # w\n",
    "                except:\n",
    "                    print(\"question:{} sql:{}\".format(self.question, self.sql))\n",
    "                # self.history.append(index_to_column_name(col, self.table))\n",
    "                # self.history.append(self.sql[key][1][0][1][0])\n",
    "                if self.sql[key][0] == \"asc\" and self.sql[\"limit\"]: # TODO: get limit value and labels\n",
    "                    label = 0\n",
    "                elif self.sql[key][0] == \"asc\" and not self.sql[\"limit\"]:\n",
    "                    label = 1\n",
    "                elif self.sql[key][0] == \"desc\" and self.sql[\"limit\"]:\n",
    "                    label = 2\n",
    "                else:\n",
    "                    label = 3                                           # agg_id in col_unit of val_unit in orderBy\n",
    "                return self.history+[index_to_column_name(col, self.table), self.sql[key][1][0][1][0]], label\n",
    "\n",
    "\n",
    "class AndOrPredictor:\n",
    "    def __init__(self, question, sql, table, history):\n",
    "        self.sql = sql\n",
    "        self.question = question\n",
    "        self.history = history\n",
    "        self.table = table\n",
    "\n",
    "    def generate_output(self):\n",
    "        if 'where' in self.sql and self.sql['where'] and len(self.sql['where']) > 1:\n",
    "            return self.history, COND_OPS[self.sql['where'][1]]\n",
    "        return self.history,-1\n",
    "    \n",
    "    \n",
    "def get_table_dict(table_data_path):\n",
    "    data = json.load(open(table_data_path))\n",
    "    table = dict()\n",
    "    for item in data:\n",
    "        table[item[\"db_id\"]] = item\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_full_history(question_tokens, sql, table, history):\n",
    "    table_schema = [\n",
    "        table[\"table_names\"],\n",
    "        table[\"column_names\"],\n",
    "        table[\"column_types\"]\n",
    "    ]\n",
    "    full_labels = []\n",
    "    masks = [[COMPONENTS_DICT['multi_sql']]]\n",
    "    stack = [(\"root\", sql)]\n",
    "    with_join = False\n",
    "    fk_dict = defaultdict(list)\n",
    "    for fk in table[\"foreign_keys\"]:\n",
    "        fk_dict[fk[0]].append(fk[1])\n",
    "        fk_dict[fk[1]].append(fk[0])\n",
    "    while len(stack) > 0:\n",
    "        node = stack.pop()\n",
    "        if node[0] == \"root\":\n",
    "            if len(node) == 3 and node[2] == \"multi\":\n",
    "                masks.append([0])\n",
    "            history, label, ret_sql = MultiSqlPredictor(question_tokens, node[1], history).generate_output()\n",
    "            full_labels.append([SQL_OPS[label]])\n",
    "            history.append(label)\n",
    "            if label == \"none\":\n",
    "                stack.append((label, ret_sql))\n",
    "                masks.append([COMPONENTS_DICT['keyword']])\n",
    "            else:\n",
    "                node[1][label] = None\n",
    "                stack.append((label, ret_sql, node[1]))\n",
    "                masks.append([COMPONENTS_DICT['multi_sql']])\n",
    "        elif node[0] in ('intersect', 'except', 'union'):\n",
    "            full_labels.append([])\n",
    "            masks.append([-1])\n",
    "            stack.append((\"root\", node[1],\"multi\"))\n",
    "            stack.append((\"root\", node[2]))\n",
    "        elif node[0] == \"none\":\n",
    "            with_join = len(node[1][\"from\"][\"table_units\"]) > 1\n",
    "            history, label, sql = KeyWordPredictor(question_tokens, node[1], history).generate_output()\n",
    "\n",
    "            label_idxs = []\n",
    "            for item in label[1]:\n",
    "                if item in KW_DICT:\n",
    "                    label_idxs.append(KW_DICT[item])\n",
    "            label_idxs.sort()\n",
    "            full_labels.append([label_idxs])\n",
    "\n",
    "            if \"orderBy\" in label[1]:\n",
    "                stack.append((\"orderBy\", node[1]))\n",
    "            if \"groupBy\" in label[1]:\n",
    "                has_having = \"having\" in label[1]\n",
    "                stack.append((\"groupBy\", node[1],has_having))\n",
    "            if \"where\" in label[1]:\n",
    "                stack.append((\"where\", node[1]))\n",
    "            if \"select\" in label[1]:\n",
    "                stack.append((\"select\", node[1]))\n",
    "\n",
    "        elif node[0] in (\"select\", \"having\", \"orderBy\"):\n",
    "            history.append(node[0])\n",
    "            masks.append([COMPONENTS_DICT['col']])\n",
    "\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, node[0]).generate_output()\n",
    "            agg_col_dict = dict()\n",
    "            op_col_dict = dict()\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"Warning: predicted 0 columns!\")\n",
    "                    continue\n",
    "                full_labels.append([get_label_cols(with_join, fk_dict, l[1])])\n",
    "                for col, sql_item in zip(l[1], s):\n",
    "                    key = \"{}{}{}\".format(col[0][0], col[0][1], col[0][2])\n",
    "                    if key not in agg_col_dict:\n",
    "                        agg_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        agg_col_dict[key].append((sql_item, col[0]))\n",
    "                    if key not in op_col_dict:\n",
    "                        op_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        op_col_dict[key].append((sql_item, col[0]))\n",
    "                for key in agg_col_dict:\n",
    "                    stack.append((\"col\", node[0], agg_col_dict[key], op_col_dict[key]))\n",
    "        elif node[0] == \"col\":\n",
    "            history.append(node[2][0][1])\n",
    "            if node[1] == \"where\":\n",
    "                stack.append((\"op\", node[2], \"where\"))\n",
    "            elif node[1] != \"groupBy\":\n",
    "                labels = []\n",
    "                for sql_item, col in node[2]:\n",
    "                    _, label = AggPredictor(question_tokens, sql_item, history, node[1]).generate_output()\n",
    "                    if label - 1 >= 0:\n",
    "                        labels.append(label - 1)\n",
    "\n",
    "                if node[1] == \"orderBy\":\n",
    "                    stack.append((\"des_asc\", sql))\n",
    "                    continue\n",
    "                masks.append([COMPONENTS_DICT['agg']])\n",
    "                full_labels.append([labels[:min(len(labels), 3)]])\n",
    "\n",
    "                if node[1] == \"having\":\n",
    "                    stack.append((\"op\", node[2], \"having\"))\n",
    "\n",
    "                if len(labels) == 0 and node[1] == \"having\":\n",
    "                    history.append(\"none\")\n",
    "                for v in labels:\n",
    "                    history.append(AGG_OPS[v + 1])\n",
    "                    if node[1] != \"having\":\n",
    "                        masks.append([-1])\n",
    "                        full_labels.append([])\n",
    "\n",
    "        elif node[0] == \"des_asc\":\n",
    "            orderby_ret = DesAscPredictor(question_tokens, node[1], table, history).generate_output()\n",
    "\n",
    "            if not orderby_ret:\n",
    "                continue\n",
    "            masks.append([COMPONENTS_DICT['des_asc']])\n",
    "            # print(node[1])\n",
    "            history.append(ORDER_DICT[orderby_ret[1]])\n",
    "            full_labels.append([orderby_ret[1]])\n",
    "            masks.append([-1])\n",
    "            full_labels.append([])\n",
    "        elif node[0] == \"value\":\n",
    "            history.append(node[2])\n",
    "            masks.append([COMPONENTS_DICT['root_tem'],COMPONENTS_DICT['value']])\n",
    "            val1 = 0 # TODO: node[1][3]\n",
    "            val2 = 1 # TODO: node[1][4]\n",
    "            full_labels.append([1, [val1, val2]])\n",
    "            history.append(\"value\") # TODO: ([val1, val2])\n",
    "            masks.append([-1])\n",
    "            full_labels.append([])\n",
    "\n",
    "        elif node[0] == \"op\":\n",
    "            # history.append(node[1][0][1])\n",
    "            labels = []\n",
    "\n",
    "            for sql_item, col in node[1]:\n",
    "                _, label, s = OpPredictor(question_tokens, sql_item, history).generate_output()\n",
    "                if label != -1:\n",
    "                    labels.append(label)\n",
    "\n",
    "                # masks.append(COMPONENTS_DICT['root_tem'])\n",
    "                if isinstance(s[0], dict):\n",
    "                    stack.append((\"root\", s[0]))\n",
    "                    history.append(NEW_WHERE_OPS[label])\n",
    "                else:\n",
    "                    stack.append((\"value\",sql_item,NEW_WHERE_OPS[label]))\n",
    "\n",
    "            if len(labels) > 2:\n",
    "                print(question_tokens)\n",
    "            masks.append([COMPONENTS_DICT['op']])\n",
    "            full_labels.append([labels])\n",
    "            if stack[-1][0] == \"root\":\n",
    "                full_labels.append([0])\n",
    "                masks.append([COMPONENTS_DICT['root_tem']])\n",
    "                masks.append([COMPONENTS_DICT['multi_sql']])\n",
    "        elif node[0] == \"where\":\n",
    "            history.append(node[0])\n",
    "            hist, andor_label = AndOrPredictor(question_tokens, node[1], table, history).generate_output()\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, \"where\").generate_output()\n",
    "            # masks.append([COMPONENTS_DICT['col']])\n",
    "            op_col_dict = dict()\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"Warning: predicted 0 columns!\")\n",
    "                    continue\n",
    "\n",
    "                label = get_label_cols(with_join, fk_dict, l[1])\n",
    "                if len(label) > 1:\n",
    "                    full_labels.append([label,[andor_label]])\n",
    "                    masks.append([COMPONENTS_DICT['col'],COMPONENTS_DICT['andor']])\n",
    "                else:\n",
    "                    full_labels.append([label,[]])\n",
    "                    masks.append([COMPONENTS_DICT['col'], COMPONENTS_DICT['andor']])\n",
    "                # full_labels.append()\n",
    "                for col, sql_item in zip(l[1], s):\n",
    "                    key = \"{}{}{}\".format(col[0][0], col[0][1], col[0][2])\n",
    "                    if key not in op_col_dict:\n",
    "                        op_col_dict[key] = [(sql_item, col[0])]\n",
    "                    else:\n",
    "                        op_col_dict[key].append((sql_item, col[0]))\n",
    "                for key in op_col_dict:\n",
    "                    stack.append((\"col\", \"where\", op_col_dict[key]))\n",
    "        elif node[0] == \"groupBy\":\n",
    "\n",
    "            history.append(node[0])\n",
    "            col_ret = ColPredictor(question_tokens, node[1], table, history, node[0]).generate_output()\n",
    "            masks.append([COMPONENTS_DICT['col']])\n",
    "            # agg_col_dict = dict()\n",
    "            for h, l, s in col_ret:\n",
    "                if l[0] == 0:\n",
    "                    print(\"Warning: predicted 0 columns!\")\n",
    "                    continue\n",
    "\n",
    "                history.append(l[1][0][0])\n",
    "                full_labels.append([get_label_cols(with_join, fk_dict, l[1])])\n",
    "                if node[2]:\n",
    "                    stack.append((\"having\", node[1]))\n",
    "                    full_labels.append([1])\n",
    "                    masks.append([COMPONENTS_DICT['having']])\n",
    "                else:\n",
    "                    full_labels.append([0])\n",
    "                    masks.append([COMPONENTS_DICT['having']])\n",
    "\n",
    "    return history,full_labels,masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_value(conditions,nl,mp):\n",
    "    for cond in conditions:\n",
    "        for i,value in enumerate(cond):\n",
    "            if i < 3:\n",
    "                continue\n",
    "            if not value or isinstance(value,dict):\n",
    "                continue\n",
    "            old_value = value\n",
    "            if isinstance(value,str):\n",
    "                if value[0] in ('\\'','\\\"'):\n",
    "                    value = value[1:-1]\n",
    "                value = value.split()\n",
    "            else:\n",
    "                value = [value]\n",
    "            try:\n",
    "                new_val = 'VALUE_{}'.format(len(mp))\n",
    "                cond[i] = new_val\n",
    "                mp.append(old_value)\n",
    "                if isinstance(value[0],str):\n",
    "                    idx = nl.index(value[0])\n",
    "                else:\n",
    "                    idx = -1\n",
    "                    for i in range(len(nl)):\n",
    "                        if nl[i].isdigit() and (float(nl[i]) == value[0]):\n",
    "                            idx = i\n",
    "                            break\n",
    "                    if idx == -1:\n",
    "                        # print(old_value)\n",
    "                        # print(nl)\n",
    "                        continue\n",
    "                nl = nl[:idx] + [new_val] + nl[idx+len(value):]\n",
    "            except Exception:\n",
    "                # print(old_value)\n",
    "                # print(nl)\n",
    "                continue\n",
    "    return conditions,nl\n",
    "\n",
    "def replace_nl(sql,nl):\n",
    "    mp = []\n",
    "    sql[\"where\"],nl = replace_value(sql[\"where\"],nl,mp)\n",
    "    sql[\"having\"],nl = replace_value(sql[\"having\"],nl,mp)\n",
    "    d = {}\n",
    "    for i,val in enumerate(mp):\n",
    "        d[\"VALUE_{}\".format(i)] = val\n",
    "    return d,sql,nl\n",
    "\n",
    "def get_table_schema(table):\n",
    "    col_names = table[\"column_names\"]\n",
    "    tab_names = table[\"table_names\"]\n",
    "    col_types = table[\"column_types\"]\n",
    "    col_name_comb = []\n",
    "    for coln, colt in zip(col_names, col_types):\n",
    "        tab_id, coln_str = coln\n",
    "        if tab_id == -1:\n",
    "            col_name_comb.append([\"all\", coln_str])\n",
    "        else:\n",
    "            col_name_comb.append(tab_names[tab_id].split(\" \") + coln_str.split(\" \") + [colt])\n",
    "    \n",
    "    return col_name_comb\n",
    "\n",
    "def get_col_in_history(history):\n",
    "    col_inds = []\n",
    "    col_mask = []\n",
    "    history_col_replaced = []\n",
    "    #[\"singer\", \"age\", 13]\n",
    "    for hs in history:\n",
    "        if isinstance(hs, tuple):\n",
    "            col_inds.append(hs[2])\n",
    "            col_mask.append(1)\n",
    "            history_col_replaced.append(\"column\")\n",
    "        else:\n",
    "            col_inds.append(-1)\n",
    "            col_mask.append(0)\n",
    "            history_col_replaced.append(hs)\n",
    "    \n",
    "    return col_inds, col_mask, history_col_replaced\n",
    "        \n",
    "            \n",
    "def parse_data_new_format(data, table_dict):\n",
    "    dataset = []\n",
    "    for item in data:\n",
    "        table_one = table_dict[item[\"db_id\"]]\n",
    "        table_schema = get_table_schema(table_one)\n",
    "        mp,sql,nl = replace_nl(item[\"sql\"],item[\"question_toks\"])\n",
    "        history, labels, masks = parse_data_full_history(nl, sql, table_one, [])\n",
    "        \n",
    "        col_inds, col_mask, history_col_replaced = get_col_in_history(history)\n",
    "        if not len(history) == len(labels) == len(masks):\n",
    "            print('\\n-------------------------------')\n",
    "            print('len of hisotry: ', len(history), 'len of labels: ', len(labels), \"len of masks: \", len(masks))\n",
    "            print(\"query  : \", item['query'])\n",
    "            print(\"history: \", history)\n",
    "            print(\"masks    : \", masks)\n",
    "            print(\"label    : \", labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS_DICT = {\n",
    "    'multi_sql':0,\n",
    "    'keyword':1,\n",
    "    'col':2,\n",
    "    'op':3,\n",
    "    'agg':4,\n",
    "    'root_tem':5,\n",
    "    'des_asc':6,\n",
    "    'having':7,\n",
    "    'andor':8,\n",
    "    'value':9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stepten no order by\n",
    "# SELECT T1.Area FROM APPELLATIONS AS T1 JOIN WINE AS T2 ON T1.Appelation  =  T2.Appelation GROUP BY T2.Appelation HAVING T2.year  <  2010 ORDER BY count(*) DESC LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "len of hisotry:  14 len of labels:  13 len of masks:  13\n",
      "query  :  SELECT t3.individual_last_name FROM organizations AS t1 JOIN organization_contact_individuals AS t2 ON t1.organization_id  =  t2.organization_id JOIN individuals AS t3 ON t2.individual_id  =  t3.individual_id WHERE t1.uk_vat_number  =  (SELECT max(uk_vat_number) FROM organizations) ORDER BY t2.date_contact_to ASC LIMIT 1\n",
      "history:  ['root', 'none', 'select', ('individuals', 'individual last name', 23), 'where', ('organizations', 'uk vat number', 27), '=', 'root', 'none', 'select', ('organizations', 'uk vat number', 27), 'max', 'orderBy', ('organization contact individuals', 'date contact to', 35)]\n",
      "masks    :  [[0], [1], [2], [4], [2, 8], [3], [5], [0], [1], [2], [4], [-1], [2]]\n",
      "label    :  [[0], [[0, 2]], [[23]], [[]], [[27], []], [[0]], [0], [0], [[]], [[27]], [[0]], [], [[35]]]\n",
      "\n",
      "-------------------------------\n",
      "len of hisotry:  14 len of labels:  13 len of masks:  13\n",
      "query  :  SELECT t3.individual_last_name FROM organizations AS t1 JOIN organization_contact_individuals AS t2 ON t1.organization_id  =  t2.organization_id JOIN individuals AS t3 ON t2.individual_id  =  t3.individual_id WHERE t1.uk_vat_number  =  (SELECT max(uk_vat_number) FROM organizations) ORDER BY t2.date_contact_to ASC LIMIT 1\n",
      "history:  ['root', 'none', 'select', ('individuals', 'individual last name', 23), 'where', ('organizations', 'uk vat number', 27), '=', 'root', 'none', 'select', ('organizations', 'uk vat number', 27), 'max', 'orderBy', ('organization contact individuals', 'date contact to', 35)]\n",
      "masks    :  [[0], [1], [2], [4], [2, 8], [3], [5], [0], [1], [2], [4], [-1], [2]]\n",
      "label    :  [[0], [[0, 2]], [[23]], [[]], [[27], []], [[0]], [0], [0], [[]], [[27]], [[0]], [], [[35]]]\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../SyntaxSQL/data/dev.json'\n",
    "table_data_path = '../../SyntaxSQL/data/tables.json'\n",
    "data = json.load(open(data_path))\n",
    "table_dict = get_table_dict(table_data_path)\n",
    "parse_data_new_format(data, table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
